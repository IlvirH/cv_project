{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4hd34WZAtD7"
      },
      "outputs": [],
      "source": [
        "!pip install gradio\n",
        "!pip install tqdm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "3MPPmjpaC7tI"
      },
      "execution_count": 393,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 394,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3Qpgh33AtEB",
        "outputId": "9db2c6cd-2c8d-4cb5-9623-cced75f3b2bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 394
        }
      ],
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 426,
      "metadata": {
        "id": "yej8W_jhAtEC"
      },
      "outputs": [],
      "source": [
        "image_size = 28\n",
        "batch_size = 128\n",
        "latent_size = 100\n",
        "num_classes = 10\n",
        "stats = (0.5,), (0.5,)\n",
        "\n",
        "# Dataset\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(*stats)\n",
        "])\n",
        "\n",
        "train_dataset = MNIST(root=\"./data\", transform=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 427,
      "metadata": {
        "id": "ugrq16qlAtEE"
      },
      "outputs": [],
      "source": [
        "class DiscriminatorModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiscriminatorModel, self).__init__()\n",
        "        input_dim = 784 + 10\n",
        "        output_dim = 1\n",
        "        self.label_embedding = nn.Embedding(10, 10)\n",
        "\n",
        "        self.hidden_layer1 = nn.Sequential(\n",
        "            nn.Linear(input_dim, 1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        self.hidden_layer2 = nn.Sequential(\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        self.hidden_layer3 = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        self.hidden_layer4 = nn.Linear(256, output_dim)\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        c = self.label_embedding(labels).view(labels.shape[0], -1)\n",
        "        x = torch.cat([x, c], 1)\n",
        "        output = self.hidden_layer1(x)\n",
        "        output = self.hidden_layer2(output)\n",
        "        output = self.hidden_layer3(output)\n",
        "        output = self.hidden_layer4(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class GeneratorModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GeneratorModel, self).__init__()\n",
        "        input_dim = 100 + 10\n",
        "        output_dim = 784\n",
        "        self.label_embedding = nn.Embedding(10, 10)\n",
        "        \n",
        "        self.hidden_layer1 = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "\n",
        "        self.hidden_layer2 = nn.Sequential(\n",
        "            nn.Linear(256, 512),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "\n",
        "        self.hidden_layer3 = nn.Sequential(\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "\n",
        "        self.hidden_layer4 = nn.Sequential(\n",
        "            nn.Linear(1024, output_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x, labels):\n",
        "        c = self.label_embedding(labels).view(labels.shape[0], -1)\n",
        "        x = torch.cat([x, c], 1)\n",
        "        output = self.hidden_layer1(x)\n",
        "        output = self.hidden_layer2(output)\n",
        "        output = self.hidden_layer3(output)\n",
        "        output = self.hidden_layer4(output)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = DiscriminatorModel()\n",
        "generator = GeneratorModel()\n",
        "discriminator.to(device)\n",
        "generator.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMXbxEdXEDoU",
        "outputId": "aa228d0d-9f4f-4c1d-f7b8-3a2a3c23c7b2"
      },
      "execution_count": 428,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GeneratorModel(\n",
              "  (label_embedding): Embedding(10, 10)\n",
              "  (hidden_layer1): Sequential(\n",
              "    (0): Linear(in_features=110, out_features=256, bias=True)\n",
              "    (1): LeakyReLU(negative_slope=0.2)\n",
              "  )\n",
              "  (hidden_layer2): Sequential(\n",
              "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
              "    (1): LeakyReLU(negative_slope=0.2)\n",
              "  )\n",
              "  (hidden_layer3): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=1024, bias=True)\n",
              "    (1): LeakyReLU(negative_slope=0.2)\n",
              "  )\n",
              "  (hidden_layer4): Sequential(\n",
              "    (0): Linear(in_features=1024, out_features=784, bias=True)\n",
              "    (1): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 428
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt_d = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "opt_g = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ],
      "metadata": {
        "id": "-bNaIft2mIRF"
      },
      "execution_count": 429,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 430,
      "metadata": {
        "id": "i6ncw7RuAtEG"
      },
      "outputs": [],
      "source": [
        "# Функция денормализации изображений\n",
        "def denorm(img_tensors):\n",
        "    return img_tensors * stats[1][0] + stats[0][0]\n",
        "\n",
        "# Функция сохранения сгенерированных изображений\n",
        "sample_dir = 'generated'\n",
        "os.makedirs(sample_dir, exist_ok=True)\n",
        "\n",
        "def save_samples(index, latent_tensors, labels, show=True):\n",
        "    latent_tensors = latent_tensors.view(latent_tensors.size(0), -1)\n",
        "    if latent_tensors.size(1) != labels.size(1):\n",
        "        labels = labels.view(labels.size(0), -1)\n",
        "    fake_images = generator(latent_tensors, labels).to(device)\n",
        "    fake_fname = 'image-{0:0=4d}.png'.format(index)\n",
        "    save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=8)\n",
        "    if show:\n",
        "        fig, ax = plt.subplots(figsize=(8, 8))\n",
        "        ax.set_xticks([]); ax.set_yticks([])\n",
        "        ax.imshow(make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))\n",
        "\n",
        "\n",
        "\n",
        "# Training functions\n",
        "def train_discriminator(real_images, labels):\n",
        "    opt_d.zero_grad()\n",
        "\n",
        "    real_images = real_images.view(-1, 784).to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    real_preds = discriminator(real_images, labels)\n",
        "    real_loss = F.binary_cross_entropy_with_logits(real_preds, torch.ones_like(real_preds))\n",
        "    real_score = torch.mean(torch.sigmoid(real_preds)).item()  # Применяем сигмоиду к real_preds\n",
        "\n",
        "    latent = torch.randn(batch_size, latent_size).to(device)\n",
        "    fake_labels = torch.randint(0, num_classes, (batch_size,)).to(device)\n",
        "    fake_images = generator(latent, fake_labels).detach().to(device)\n",
        "\n",
        "    fake_preds = discriminator(fake_images, fake_labels)\n",
        "    fake_loss = F.binary_cross_entropy_with_logits(fake_preds, torch.zeros_like(fake_preds))\n",
        "    fake_score = torch.mean(torch.sigmoid(fake_preds)).item()  # Применяем сигмоиду к fake_preds\n",
        "\n",
        "    loss = real_loss + fake_loss\n",
        "    loss.backward()\n",
        "    opt_d.step()\n",
        "\n",
        "    return loss.item(), real_score, fake_score\n",
        "\n",
        "def train_generator():\n",
        "    opt_g.zero_grad()\n",
        "\n",
        "    latent = torch.randn(batch_size, latent_size).to(device)\n",
        "    labels = torch.randint(0, num_classes, (batch_size,)).to(device)\n",
        "\n",
        "    fake_labels = labels\n",
        "    fake_images = generator(latent, fake_labels)\n",
        "    fake_preds = discriminator(fake_images, fake_labels)\n",
        "    loss = F.binary_cross_entropy_with_logits(fake_preds, torch.ones_like(fake_preds))\n",
        "    loss.backward()\n",
        "    opt_g.step()\n",
        "\n",
        "    return loss.item(), latent\n",
        "\n",
        "def fit(epochs=10, start_idx=1):\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    losses_g = []\n",
        "    losses_d = []\n",
        "    real_scores = []\n",
        "    fake_scores = []\n",
        "\n",
        "    for epoch in range(epochs):  # Iterate over the range of epochs\n",
        "        for real_images, labels in tqdm(train_loader):\n",
        "            real_images = real_images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            loss_d, real_score, fake_score = train_discriminator(real_images, labels)\n",
        "            loss_g, latent = train_generator()\n",
        "\n",
        "        losses_g.append(loss_g)\n",
        "        losses_d.append(loss_d)\n",
        "        real_scores.append(real_score)\n",
        "        fake_scores.append(fake_score)\n",
        "\n",
        "        print(f\"[{epoch+1}/{epochs}], loss_g: {loss_g:.4f}, loss_d: {loss_d:.4f}, real_score: {real_score:.4f}, fake_score: {fake_score:.4f}\")\n",
        "\n",
        "        # save_samples(epoch+start_idx, latent, labels, show=False)\n",
        "\n",
        "    return losses_g, losses_d, latent, fake_scores\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 431,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDtNfXtVAtEH",
        "outputId": "9b931ed7-7d0f-4622-f04f-7a331459f020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:53<00:00,  8.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/12], loss_g: 1.7666, loss_d: 0.3318, real_score: 0.8137, fake_score: 0.0838\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:53<00:00,  8.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2/12], loss_g: 4.0560, loss_d: 0.2708, real_score: 0.9425, fake_score: 0.1561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:53<00:00,  8.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3/12], loss_g: 2.9062, loss_d: 0.4146, real_score: 0.9653, fake_score: 0.2488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:54<00:00,  8.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4/12], loss_g: 2.2213, loss_d: 0.5825, real_score: 0.8057, fake_score: 0.1651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:54<00:00,  8.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5/12], loss_g: 1.5142, loss_d: 0.6921, real_score: 0.6629, fake_score: 0.0819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:54<00:00,  8.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6/12], loss_g: 1.8435, loss_d: 0.5887, real_score: 0.8036, fake_score: 0.1921\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:53<00:00,  8.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7/12], loss_g: 2.3665, loss_d: 0.5687, real_score: 0.8537, fake_score: 0.2447\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:54<00:00,  8.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8/12], loss_g: 1.2331, loss_d: 0.6864, real_score: 0.6867, fake_score: 0.1320\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:53<00:00,  8.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9/12], loss_g: 1.9921, loss_d: 0.7326, real_score: 0.7865, fake_score: 0.3134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:54<00:00,  8.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10/12], loss_g: 1.4760, loss_d: 0.9328, real_score: 0.6470, fake_score: 0.2670\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:53<00:00,  8.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11/12], loss_g: 2.0437, loss_d: 1.2769, real_score: 0.8015, fake_score: 0.5496\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 469/469 [00:53<00:00,  8.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12/12], loss_g: 1.5111, loss_d: 0.9639, real_score: 0.6518, fake_score: 0.2960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "losses_g, losses_d, latent, fake_scores = fit(epochs=12, start_idx=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 432,
      "metadata": {
        "id": "nJbtwBXsAtEI"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Сохраняем обученную модель\n",
        "torch.save(generator.state_dict(), 'CGAN.pth')\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}